---
title: 'Harms in Machine Learning'
date: 2025-10-30
permalink: /posts/2025/10/blog-post-8/
tags:
  - case study
  - ethics
  - machine learning
  - AI
---

Machine learning algorithms can inherit biases from many points in the modeling process ranging from data collection to deployment. 

**Case Study reading:**  
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle]([https://arstechnica.com/science/2025/09/these-psychological-tricks-can-get-llms-to-respond-to-forbidden-prompts/](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2))

Discussion Questions
---
I think this article was really great overall in categorizing ways that bias can be introduced into models. One thing I would add though, is related to the use of training, validation and testing data. They mentioned "the test data is not used before this step, to ensure that the modelâ€™s performance is a true representation of how it performs on unseen data." This is what should be done. However, in practice there can be data leakage from the training set, which does introduce bias into the model. It will be less generalizable to the overall population if information from the test set is leaked during training. 

In my machine learning project where I attempted to classify lichen images from iNaturalist, I experienced a lot of difficulty with the data and representation. Lichens are notoriously difficult to identify even by non-AI, so I had some species that had a lot of observations and some that had very few observations. This could be an example of representation bias. The main issue I had when training my model is that the model preferentially picked the largest 1 or 2 classes and ignored the others because they were so small. 

Developers should obviously be more mindful of bias in their algorithms. I'm going to use the example of a facial recognition system to identify what steps we can take to mititgate harm.
Data Collection:
- Collect image data from a diverse population including race, gender, age and disability status.
- Using data from a law enforcement data could cause historical bias.
Data Preparation:
- Augment data if necessary
- Ensure image processing is applicable to all groups (ex. exposure)
Model Development:
- Model will likely try to minimize error by predicting the majority group (white men), don't necessary optimize accuracy
- Quarantine test set
Model Evaluation:
- Evaluate metrics by group
Model Deployment:
- Consider usage of model: is it ethical?
- Is it used for surveillance, identification, etc.

## New Question
Should algorithms be able to pass standards regarding bias before being implemented? 

## Reflection
I really liked this article because it included a lot of biases we've already talked about but described them more thoroughly. I think historical bias is probably the hardest to deal with as someone creating an algorithm because when people behind the data are biased how can you even get unbiased data? That to me seems like the most challenging problem. 
